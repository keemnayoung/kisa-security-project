#!/bin/bash
# ============================================================
# run.sh - KISA ë³´ì•ˆ ì·¨ì•½ì  ì ê²€ ì‹œìŠ¤í…œ í†µí•© ì‹¤í–‰
#
#   ./run.sh scan       â†’ OS ì ê²€ + íŒŒì‹± + ì •ë¦¬
#   ./run.sh scan-db    â†’ DB ì ê²€ + íŒŒì‹± + ì •ë¦¬
#   ./run.sh scan-all   â†’ OS + DB ì ê²€ + íŒŒì‹± + ì •ë¦¬
#   ./run.sh fix        â†’ OS ì¡°ì¹˜ + íŒŒì‹± + ì •ë¦¬
#   ./run.sh fix-db     â†’ DB ì¡°ì¹˜ + íŒŒì‹± + ì •ë¦¬
#   ./run.sh score      â†’ ë³´ì•ˆ ì ìˆ˜ ê³„ì‚°
#   ./run.sh dashboard  â†’ ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
#   ./run.sh api        â†’ ë‚´ë¶€ë§ Job API(FastAPI) ì‹¤í–‰
#   ./run.sh all        â†’ ì „ì²´ ì ê²€ + íŒŒì‹± + ëŒ€ì‹œë³´ë“œ
#   ./run.sh mock       â†’ ê°€ì§œ ë°ì´í„° ìƒì„± + DB ì ìš©
# ============================================================

PROJECT_DIR="$(cd "$(dirname "$0")" && pwd)"
VENV_DIR="$PROJECT_DIR/venv"

ensure_local_audit_dirs() {
    # Ansible fetch writes to /tmp/audit/{check,fix}. If these dirs are root-owned
    # (happens when cleanup used sudo or a previous run created them), fetch will fail
    # and pipeline will see "no JSON".
    local base="/tmp/audit"
    local dirs=("${base}" "${base}/check" "${base}/fix")

    for d in "${dirs[@]}"; do
        mkdir -p "$d" 2>/dev/null || true
    done

    # If not writable, try to fix with passwordless sudo (common in this project setup).
    for d in "${dirs[@]}"; do
        if [ -w "$d" ]; then
            continue
        fi

        if command -v sudo >/dev/null 2>&1 && sudo -n true 2>/dev/null; then
            sudo -n mkdir -p "$d" 2>/dev/null || true
            sudo -n chown -R "$(id -u)":"$(id -g)" "$base" 2>/dev/null || true
            sudo -n chmod -R u+rwX,go+rX "$base" 2>/dev/null || true
        fi
    done
}

ansible_playbook() {
    local playbook_path="$1"
    shift || true

    local inventory_path="${ANSIBLE_INVENTORY:-inventories/hosts.ini}"
    local vault_pass_file="$PROJECT_DIR/ansible/.vault_pass"
    local args=(-i "${inventory_path}")

    # Ansible Vault ë¹„ë°€ë²ˆí˜¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ ì‚¬ìš©
    if [[ -f "${vault_pass_file}" ]]; then
        args+=(--vault-password-file "${vault_pass_file}")
    fi

    if [[ -n "${ANSIBLE_LIMIT:-}" ]]; then
        args+=(--limit "${ANSIBLE_LIMIT}")
    fi
    if [[ -n "${ANSIBLE_EXTRA_VARS_FILE:-}" ]]; then
        args+=(-e "@${ANSIBLE_EXTRA_VARS_FILE}")
    fi

    ansible-playbook "${args[@]}" "${playbook_path}" "$@"
}

activate_venv() {
    if [ -f "$VENV_DIR/bin/activate" ]; then
        source "$VENV_DIR/bin/activate"
    else
        echo "[ERROR] ê°€ìƒí™˜ê²½ì´ ì—†ìŠµë‹ˆë‹¤."
        echo "  python3 -m venv venv && source venv/bin/activate && pip install -r requirements.txt"
        exit 1
    fi
}

sync_inventory() {
    # Generate ansible/inventories/hosts.ini + group_vars from DB.
    # This must run inside the venv because DB connector deps are installed there.
    activate_venv
    (cd "$PROJECT_DIR/backend" && python3 sync_inventory.py)
}

cleanup_tmp_dir() {
    local target_dir="$1"

    if [ ! -d "$target_dir" ]; then
        echo "[OK] ${target_dir}/ ì •ë¦¬ ì™„ë£Œ (ë””ë ‰í† ë¦¬ ì—†ìŒ)"
        return 0
    fi

    if rm -rf "${target_dir}"/* 2>/dev/null; then
        echo "[OK] ${target_dir}/ ì •ë¦¬ ì™„ë£Œ"
        return 0
    fi

    if command -v sudo >/dev/null 2>&1 && sudo -n true 2>/dev/null; then
        if sudo -n rm -rf "${target_dir}"/* 2>/dev/null; then
            echo "[OK] ${target_dir}/ ì •ë¦¬ ì™„ë£Œ (sudo ì‚¬ìš©)"
            return 0
        fi
    fi

    echo "[WARN] ${target_dir}/ ì¼ë¶€ íŒŒì¼ ì •ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”."
    return 1
}

maybe_cleanup_tmp_dir() {
    local target_dir="$1"
    if [[ "${SKIP_CLEANUP:-0}" == "1" ]]; then
        echo "[OK] ${target_dir}/ ì •ë¦¬ ê±´ë„ˆëœ€ (SKIP_CLEANUP=1)"
        return 0
    fi
    cleanup_tmp_dir "$target_dir"
}

_split_csv() {
    # Split comma-separated values into newline-separated tokens.
    # Usage: _split_csv "a,b,c"
    local s="${1:-}"
    if [[ -z "$s" ]]; then
        return 0
    fi
    echo "$s" | tr ',' '\n' | awk 'NF{print $0}'
}

_inventory_prefixes_for_limit() {
    # Build filename prefixes (<company>_<server_id>_) for hosts in ANSIBLE_LIMIT.
    # This is used to prune stale JSON results that would otherwise be parsed into DB.
    local inventory_path="${ANSIBLE_INVENTORY:-inventories/hosts.ini}"
    local limit="${ANSIBLE_LIMIT:-}"

    if [[ -z "$limit" ]]; then
        return 1
    fi
    if [[ ! -f "$inventory_path" ]]; then
        return 1
    fi

    local host
    while IFS= read -r host; do
        # If user passed a group name/wildcard, we can't safely map -> do nothing.
        if [[ "$host" =~ [*:?\\[] ]]; then
            return 1
        fi
        # Match an inventory host line: <host> ... company=... server_id=...
        local line
        line="$(awk -v h="$host" '$1==h{print; exit 0}' "$inventory_path")"
        if [[ -z "$line" ]]; then
            # Might be a group name; don't prune in that case.
            return 1
        fi

        local company server_id
        company="$(echo "$line" | sed -n 's/.*company=\\([^ ]*\\).*/\\1/p')"
        server_id="$(echo "$line" | sed -n 's/.*server_id=\\([^ ]*\\).*/\\1/p')"
        if [[ -z "$company" || -z "$server_id" ]]; then
            return 1
        fi

        echo "${company}_${server_id}_"
    done < <(_split_csv "$limit")

    return 0
}

_inventory_server_ids_for_limit() {
    # Build server_id list for hosts in ANSIBLE_LIMIT.
    local inventory_path="${ANSIBLE_INVENTORY:-inventories/hosts.ini}"
    local limit="${ANSIBLE_LIMIT:-}"

    if [[ -z "$limit" ]]; then
        return 1
    fi
    if [[ ! -f "$inventory_path" ]]; then
        return 1
    fi

    local host
    while IFS= read -r host; do
        if [[ "$host" =~ [*:?\\[] ]]; then
            return 1
        fi
        local line
        line="$(awk -v h="$host" '$1==h{print; exit 0}' "$inventory_path")"
        if [[ -z "$line" ]]; then
            return 1
        fi
        local server_id
        server_id="$(echo "$line" | sed -n 's/.*server_id=\\([^ ]*\\).*/\\1/p')"
        if [[ -z "$server_id" ]]; then
            return 1
        fi
        echo "$server_id"
    done < <(_split_csv "$limit")

    return 0
}

export_pipeline_server_filter() {
    # Tell the python parser which server_id(s) are allowed for this run.
    # Even if stale JSON exists, it won't be inserted into DB.
    local ids
    ids="$(_inventory_server_ids_for_limit 2>/dev/null | paste -sd, - 2>/dev/null || true)"
    if [[ -n "${ids:-}" ]]; then
        export PIPELINE_ALLOWED_SERVER_IDS="$ids"
    else
        unset PIPELINE_ALLOWED_SERVER_IDS || true
    fi
}

normalize_ansible_limit_server_ids() {
    # If ANSIBLE_LIMIT contains server_id(s) (e.g., naver-r9-002), translate them to
    # actual inventory hostnames after sync_inventory regenerates hosts.ini.
    local inventory_path="${ANSIBLE_INVENTORY:-$PROJECT_DIR/ansible/inventories/hosts.ini}"
    local limit="${ANSIBLE_LIMIT:-}"
    if [[ -z "$limit" ]]; then
        return 0
    fi
    if [[ ! -f "$inventory_path" ]]; then
        return 0
    fi

    local out=()
    local tok
    while IFS= read -r tok; do
        [[ -z "$tok" ]] && continue
        # Leave groups/wildcards untouched.
        if [[ "$tok" =~ [*:?\\[] ]]; then
            out+=("$tok")
            continue
        fi
        # If token already matches a host, keep it.
        if awk -v h="$tok" '$1==h{found=1} END{exit(found?0:1)}' "$inventory_path" 2>/dev/null; then
            out+=("$tok")
            continue
        fi
        # Heuristic: if token looks like a server_id, map it to a host with server_id=...
        if [[ "$tok" == *-* ]]; then
            local mapped
            mapped="$(awk -v sid="$tok" '
                $0 ~ /^[[:space:]]*($|#|\\[)/ {next}
                {for (i=2;i<=NF;i++) if ($i=="server_id="sid) {print $1; exit 0}}
            ' "$inventory_path" 2>/dev/null)"
            if [[ -n "$mapped" ]]; then
                out+=("$mapped")
                continue
            fi
        fi
        out+=("$tok")
    done < <(_split_csv "$limit")

    if [[ "${#out[@]}" -gt 0 ]]; then
        ANSIBLE_LIMIT="$(IFS=,; echo "${out[*]}")"
        export ANSIBLE_LIMIT
    fi
}

prune_scan_dir_for_limit() {
    # Keep only result JSONs that belong to hosts in this run (by company/server_id prefix).
    # Prevents "I scanned 002 but 001 also appears" due to stale files in the scan output dir.
    local dir="${1:-${SCAN_OUTPUT_DIR:-/tmp/audit/check}}"
    if [[ ! -d "$dir" ]]; then
        return 0
    fi

    local prefixes=()
    local p
    while IFS= read -r p; do
        [[ -n "$p" ]] && prefixes+=("$p")
    done < <(_inventory_prefixes_for_limit) || true

    if [[ "${#prefixes[@]}" -eq 0 ]]; then
        # No safe mapping; don't prune.
        return 0
    fi

    local f base keep
    shopt -s nullglob
    for f in "$dir"/*.json; do
        base="$(basename "$f")"
        keep=0
        for p in "${prefixes[@]}"; do
            if [[ "$base" == "${p}"* ]]; then
                keep=1
                break
            fi
        done
        if [[ "$keep" -eq 0 ]]; then
            rm -f "$f" 2>/dev/null || true
        fi
    done
    shopt -u nullglob
}

make_scan_output_dir() {
    # Create an isolated per-run output directory to avoid mixing stale JSONs.
    local base="/tmp/audit/check_runs"
    local ts
    ts="$(date +%Y%m%d_%H%M%S)"
    local dir="${base}/${ts}"
    mkdir -p "$dir" 2>/dev/null || true
    echo "$dir"
}

run_scan() {
    echo "=============================================="
    echo "  [1/3] OS ì·¨ì•½ì  ì ê²€ ì‹¤í–‰"
    echo "=============================================="
    ensure_local_audit_dirs
    export SCAN_OUTPUT_DIR
    SCAN_OUTPUT_DIR="$(make_scan_output_dir)"
    cd "$PROJECT_DIR/ansible"
    ansible_playbook playbooks/scan_os.yml -e "scan_output_dir=${SCAN_OUTPUT_DIR}"

    echo ""
    echo "=============================================="
    echo "  [2/3] ì ê²€ ê²°ê³¼ DB ì €ì¥"
    echo "=============================================="
    prune_scan_dir_for_limit "${SCAN_OUTPUT_DIR}"
    export_pipeline_server_filter
    activate_venv
    cd "$PROJECT_DIR/backend"
    python3 run_pipeline.py scan

    echo ""
    echo "=============================================="
    echo "  [3/3] ì„ì‹œ íŒŒì¼ ì •ë¦¬"
    echo "=============================================="
    # Keep last runner log outside /tmp/audit/check so we can debug missing items (e.g., U-64).
    if [ -f "${SCAN_OUTPUT_DIR}/os_check_runner.log" ]; then
        cp -f "${SCAN_OUTPUT_DIR}/os_check_runner.log" /tmp/audit/last_os_check_runner.log 2>/dev/null || true
    fi
    maybe_cleanup_tmp_dir "${SCAN_OUTPUT_DIR}"

    echo ""
    echo "âœ… ì ê²€ ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: ./run.sh dashboard"
}

load_fix_target_server() {
    # fix_service.pyê°€ ì €ì¥í•œ ëŒ€ìƒ ì„œë²„ IDë¥¼ ì½ì–´ ANSIBLE_LIMIT ì„¤ì •
    # ì‹ ê·œ í¬ë§·: {"server_ids": ["s1","s2"]}  /  ê¸°ì¡´ í¬ë§·: {"server_id": "s1"}
    local target_file="/tmp/audit/fix_target_server.json"
    if [[ -f "$target_file" ]]; then
        local limit
        limit="$(python3 -c "
import json
data = json.load(open('$target_file'))
if 'server_ids' in data and data['server_ids']:
    print(','.join(data['server_ids']))
elif 'server_id' in data:
    print(data['server_id'])
" 2>/dev/null || true)"
        if [[ -n "$limit" ]]; then
            export ANSIBLE_LIMIT="$limit"
            echo "[INFO] ì¡°ì¹˜ ëŒ€ìƒ ì„œë²„: $limit"
        fi
    fi
}

run_fix() {
    echo "=============================================="
    echo "  [1/3] OS ì·¨ì•½ì  ì¡°ì¹˜ ì‹¤í–‰"
    echo "=============================================="
    ensure_local_audit_dirs
    # ì´ì „ ì‹¤í–‰ ê²°ê³¼ íŒŒì¼ ì •ë¦¬ (ì¤‘ë³µ íŒŒì‹± ë°©ì§€)
    rm -f /tmp/audit/fix/*.json /tmp/audit/fix/*.tar.gz 2>/dev/null || true
    sync_inventory
    load_fix_target_server
    normalize_ansible_limit_server_ids
    cd "$PROJECT_DIR/ansible"
    ansible_playbook playbooks/fix_os.yml

    echo ""
    echo "=============================================="
    echo "  [2/3] ì¡°ì¹˜ ê²°ê³¼ DB ì €ì¥"
    echo "=============================================="
    export_pipeline_server_filter
    activate_venv
    cd "$PROJECT_DIR/backend"
    python3 run_pipeline.py fix

    echo ""
    echo "=============================================="
    echo "  [3/3] ì„ì‹œ íŒŒì¼ ì •ë¦¬"
    echo "=============================================="
    maybe_cleanup_tmp_dir /tmp/audit/fix
    # fix_target_server.json / fix_item_codes.jsonì€ fix-db jobì´ ê³µìœ í•˜ë¯€ë¡œ ì‚­ì œí•˜ì§€ ì•ŠìŒ
    # start_fix()ê°€ ë§¤ë²ˆ ë®ì–´ì“°ë¯€ë¡œ stale ìœ„í—˜ ì—†ìŒ

    echo ""
    echo "âœ… ì¡°ì¹˜ ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: ./run.sh dashboard"
}

run_score() {
    activate_venv
    cd "$PROJECT_DIR/backend"
    python3 run_pipeline.py score $1
}

run_dashboard() {
    echo "=============================================="
    echo "  ğŸ”’ SECURITYCORE ëŒ€ì‹œë³´ë“œ ì‹¤í–‰"
    echo "=============================================="
    activate_venv
    cd "$PROJECT_DIR/dashboard"
    DASHBOARD_HOST="${DASHBOARD_HOST:-127.0.0.1}"
    DASHBOARD_PORT="${DASHBOARD_PORT:-8501}"
    streamlit run Dashboard.py --server.address "$DASHBOARD_HOST" --server.port "$DASHBOARD_PORT"
}

run_api() {
    echo "=============================================="
    echo "  ğŸ”’ SECURITYCORE Job API(FastAPI) ì‹¤í–‰"
    echo "=============================================="
    activate_venv
    cd "$PROJECT_DIR"
    # Default bind is localhost to avoid bind failures in restricted environments.
    # Override with API_HOST/API_PORT if you need LAN exposure.
    API_HOST="${API_HOST:-127.0.0.1}"
    API_PORT="${API_PORT:-8000}"
    uvicorn api.main:app --host "$API_HOST" --port "$API_PORT"
}

run_mock() {
    echo "=============================================="
    echo "  ğŸ­ ê°€ì§œ ì„œë²„ ë°ì´í„° ìƒì„±"
    echo "=============================================="
    activate_venv
    cd "$PROJECT_DIR/simulation"
    python3 mock_generator.py --apply
    echo ""
    echo "âœ… ê°€ì§œ ë°ì´í„° ì ìš© ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: ./run.sh dashboard"
}

run_scan_db() {
    if [[ "${DEBUG_SCAN_DB:-0}" == "1" ]]; then
        echo "DEBUG: ANSIBLE_EXTRA_VARS_FILE=${ANSIBLE_EXTRA_VARS_FILE:-not_set}"
        if [[ -n "${ANSIBLE_EXTRA_VARS_FILE:-}" ]]; then
            echo "DEBUG: extra vars file content:"
            cat "${ANSIBLE_EXTRA_VARS_FILE}"
        fi
    fi
    
    sync_inventory
    normalize_ansible_limit_server_ids
    echo "=============================================="
    echo "  [1/3] DB ì·¨ì•½ì  ì ê²€ ì‹¤í–‰"
    echo "=============================================="
    ensure_local_audit_dirs
    export SCAN_OUTPUT_DIR
    SCAN_OUTPUT_DIR="$(make_scan_output_dir)"
    cd "$PROJECT_DIR/ansible"
    ansible_playbook playbooks/scan_db.yml -e "scan_output_dir=${SCAN_OUTPUT_DIR}"

    echo ""
    echo "=============================================="
    echo "  [2/3] ì ê²€ ê²°ê³¼ DB ì €ì¥"
    echo "=============================================="
    prune_scan_dir_for_limit "${SCAN_OUTPUT_DIR}"
    export_pipeline_server_filter
    activate_venv
    cd "$PROJECT_DIR/backend"
    python3 run_pipeline.py scan

    echo ""
    echo "=============================================="
    echo "  [3/3] ì„ì‹œ íŒŒì¼ ì •ë¦¬"
    echo "=============================================="
    if [ -f "${SCAN_OUTPUT_DIR}/db_check_runner.log" ]; then
        cp -f "${SCAN_OUTPUT_DIR}/db_check_runner.log" /tmp/audit/last_db_check_runner.log 2>/dev/null || true
    fi
    maybe_cleanup_tmp_dir "${SCAN_OUTPUT_DIR}"

    echo ""
    echo "âœ… DB ì ê²€ ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: ./run.sh dashboard"
}

run_scan_all() {
    echo "=============================================="
    echo "  ğŸ”’ ì „ì²´ ì ê²€ (OS + DB)"
    echo "=============================================="
    ensure_local_audit_dirs
    export SCAN_OUTPUT_DIR
    SCAN_OUTPUT_DIR="$(make_scan_output_dir)"
    cd "$PROJECT_DIR/ansible"
    ansible_playbook playbooks/scan_os.yml -e "scan_output_dir=${SCAN_OUTPUT_DIR}"
    ansible_playbook playbooks/scan_db.yml -e "scan_output_dir=${SCAN_OUTPUT_DIR}"

    echo ""
    echo "=============================================="
    echo "  ì ê²€ ê²°ê³¼ DB ì €ì¥"
    echo "=============================================="
    prune_scan_dir_for_limit "${SCAN_OUTPUT_DIR}"
    export_pipeline_server_filter
    activate_venv
    cd "$PROJECT_DIR/backend"
    python3 run_pipeline.py scan

    maybe_cleanup_tmp_dir "${SCAN_OUTPUT_DIR}"
    echo ""
    echo "âœ… ì „ì²´ ì ê²€ ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: ./run.sh dashboard"
}

run_fix_db() {
    echo "=============================================="
    echo "  [1/3] DB ì·¨ì•½ì  ì¡°ì¹˜ ì‹¤í–‰"
    echo "=============================================="
    ensure_local_audit_dirs
    # ì´ì „ ì‹¤í–‰ ê²°ê³¼ íŒŒì¼ ì •ë¦¬ (ì¤‘ë³µ íŒŒì‹± ë°©ì§€)
    rm -f /tmp/audit/fix/*.json /tmp/audit/fix/*.tar.gz 2>/dev/null || true
    sync_inventory
    load_fix_target_server
    normalize_ansible_limit_server_ids
    cd "$PROJECT_DIR/ansible"
    ansible_playbook playbooks/fix_db.yml

    echo ""
    echo "=============================================="
    echo "  [2/3] ì¡°ì¹˜ ê²°ê³¼ DB ì €ì¥"
    echo "=============================================="
    export_pipeline_server_filter
    activate_venv
    cd "$PROJECT_DIR/backend"
    python3 run_pipeline.py fix

    echo ""
    echo "=============================================="
    echo "  [3/3] ì„ì‹œ íŒŒì¼ ì •ë¦¬"
    echo "=============================================="
    maybe_cleanup_tmp_dir /tmp/audit/fix
    # fix_target_server.json / fix_item_codes.jsonì€ start_fix()ê°€ ë§¤ë²ˆ ë®ì–´ì“°ë¯€ë¡œ ì‚­ì œ ë¶ˆí•„ìš”

    echo ""
    echo "âœ… DB ì¡°ì¹˜ ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: ./run.sh dashboard"
}

run_all() {
    run_scan_all
    echo ""
    run_dashboard
}

show_help() {
    echo "ğŸ”’ SECURITYCORE - KISA ë³´ì•ˆ ì·¨ì•½ì  ì ê²€ ì‹œìŠ¤í…œ"
    echo ""
    echo "ì‚¬ìš©ë²•: ./run.sh [ëª…ë ¹ì–´]"
    echo ""
    echo "  scan         OS ì ê²€ + DB ì €ì¥ + ì •ë¦¬"
    echo "  scan-db      DB ì ê²€ + DB ì €ì¥ + ì •ë¦¬"
    echo "  scan-all     OS + DB ì ê²€ + DB ì €ì¥ + ì •ë¦¬"
    echo "  fix          OS ì¡°ì¹˜ + DB ì €ì¥ + ì •ë¦¬"
    echo "  fix-db       DB ì¡°ì¹˜ + DB ì €ì¥ + ì •ë¦¬"
    echo "  score [ì„œë²„] ë³´ì•ˆ ì ìˆ˜ ê³„ì‚°"
    echo "  dashboard    ëŒ€ì‹œë³´ë“œ ì‹¤í–‰"
    echo "  api          Job API(FastAPI) ì‹¤í–‰"
    echo "  all          ì „ì²´ ì ê²€ + DB ì €ì¥ + ëŒ€ì‹œë³´ë“œ"
    echo "  mock         ê°€ì§œ ë°ì´í„° ìƒì„± + DB ì ìš©"
    echo ""
}

case "${1}" in
    scan)      run_scan ;;
    scan-db)   run_scan_db ;;
    scan-all)  run_scan_all ;;
    fix)       run_fix ;;
    fix-db)    run_fix_db ;;
    score)     run_score "$2" ;;
    dashboard) run_dashboard ;;
    api)      run_api ;;
    all)       run_all ;;
    mock)      run_mock ;;
    *)         show_help ;;
esac
